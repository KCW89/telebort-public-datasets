{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "id": "title-main",
      "type": "text",
      "x": 50,
      "y": 20,
      "width": 900,
      "height": 45,
      "fontSize": 36,
      "fontFamily": 1,
      "text": "AI-2.5 Project 5: Text Generation with RLHF",
      "textAlign": "center",
      "verticalAlign": "top",
      "baseline": 32,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "subtitle",
      "type": "text",
      "x": 50,
      "y": 75,
      "width": 900,
      "height": 25,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "3-Stage Pipeline: SFT ‚Üí Reward Model ‚Üí PPO | Align LLMs with Human Preferences",
      "textAlign": "center",
      "verticalAlign": "top",
      "baseline": 18,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 80
    },
    {
      "id": "section1-header",
      "type": "rectangle",
      "x": 50,
      "y": 130,
      "width": 900,
      "height": 50,
      "strokeColor": "#f08c00",
      "backgroundColor": "#f08c00",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "section1-title",
      "type": "text",
      "x": 60,
      "y": 145,
      "width": 880,
      "height": 25,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "1Ô∏è‚É£ FILE MAP: Project Structure & 3-Stage RLHF Pipeline",
      "textAlign": "left",
      "verticalAlign": "middle",
      "baseline": 18,
      "strokeColor": "#121212",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "filemap-box",
      "type": "rectangle",
      "x": 50,
      "y": 190,
      "width": 900,
      "height": 400,
      "strokeColor": "#f08c00",
      "backgroundColor": "#1a1a1a",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "filemap-left",
      "type": "text",
      "x": 70,
      "y": 210,
      "width": 400,
      "height": 360,
      "fontSize": 14,
      "fontFamily": 3,
      "text": "üìÇ project-05-text-generation-with-rlhf/\n‚îú‚îÄ‚îÄ üìì project-05-text-generation-with-rlhf.ipynb\n‚îÇ   ‚îú‚îÄ‚îÄ Section 1: Model & Data Loading ‚úÖ\n‚îÇ   ‚îú‚îÄ‚îÄ Section 2: Stage 1 - SFT Training\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TODO 1: SFT Loss (30 min)\n‚îÇ   ‚îú‚îÄ‚îÄ Section 3: Stage 2 - Reward Model\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TODO 2: Bradley-Terry Loss (45 min)\n‚îÇ   ‚îú‚îÄ‚îÄ Section 4: Stage 3 - PPO Training\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TODO 3: KL Penalty (1.5 hrs)\n‚îÇ   ‚îú‚îÄ‚îÄ Section 5: Evaluation Suite\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TODO 4: Automated Eval (1 hr)\n‚îÇ   ‚îú‚îÄ‚îÄ Section 6: Comparison Dashboard ‚úÖ\n‚îÇ   ‚îú‚îÄ‚îÄ Section 7: Interactive Demo ‚úÖ\n‚îÇ   ‚îî‚îÄ‚îÄ Section 8: Reward Hacking Analysis ‚úÖ\n‚îÇ\n‚îú‚îÄ‚îÄ üêç config.py (Hyperparameters)\n‚îÇ   ‚îú‚îÄ‚îÄ SFTConfig (lr=2e-5, 3 epochs)\n‚îÇ   ‚îú‚îÄ‚îÄ RewardModelConfig (lr=1e-5, 1 epoch)\n‚îÇ   ‚îî‚îÄ‚îÄ PPOTrainingConfig (Œ≤=0.05, lr=1e-6)\n‚îÇ\n‚îú‚îÄ‚îÄ üîß utils.py (Helper Functions)\n‚îÇ   ‚îú‚îÄ‚îÄ generate_preference_pairs()\n‚îÇ   ‚îú‚îÄ‚îÄ calculate_metrics()\n‚îÇ   ‚îú‚îÄ‚îÄ detect_reward_hacking()\n‚îÇ   ‚îú‚îÄ‚îÄ plot_training_curves()\n‚îÇ   ‚îî‚îÄ‚îÄ calculate_win_rate()\n‚îÇ\n‚îî‚îÄ‚îÄ üìã requirements.txt\n    ‚îî‚îÄ‚îÄ transformers, trl, datasets, peft",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 345,
      "strokeColor": "#2f9e44",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "filemap-right-box",
      "type": "rectangle",
      "x": 500,
      "y": 210,
      "width": 430,
      "height": 360,
      "strokeColor": "#1971c2",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "filemap-right",
      "type": "text",
      "x": 520,
      "y": 220,
      "width": 390,
      "height": 330,
      "fontSize": 14,
      "fontFamily": 1,
      "text": "üéØ 3-Stage RLHF Pipeline:\n\nüìö STAGE 1: Supervised Fine-Tuning (SFT)\nGoal: Teach model to follow instructions\nDataset: Alpaca (52K examples)\nOutput: Instruction-following base model\n\nüèÜ STAGE 2: Reward Model Training\nGoal: Learn human preferences\nDataset: Anthropic HH-RLHF (160K pairs)\nFormula: -log(œÉ(r_chosen - r_rejected))\nOutput: Reward scorer (>65% accuracy)\n\nüöÄ STAGE 3: PPO Fine-Tuning\nGoal: Optimize for reward + stay close to SFT\nFormula: reward - Œ≤ * KL(policy || ref)\nŒ≤ = 0.05 (KL penalty coefficient)\nOutput: Aligned RLHF model\n\n‚úÖ EVALUATION:\nWin Rate: RLHF vs SFT (target ‚â•75%)\nMetrics: Perplexity, KL divergence, reward\nDemo: Gradio chatbot (3-model comparison)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 315,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "section2-header",
      "type": "rectangle",
      "x": 50,
      "y": 630,
      "width": 900,
      "height": 50,
      "strokeColor": "#1971c2",
      "backgroundColor": "#1971c2",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "section2-title",
      "type": "text",
      "x": 60,
      "y": 645,
      "width": 880,
      "height": 25,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "2Ô∏è‚É£ DATA FLOW: RLHF Pipeline - From Raw Model to Aligned Assistant",
      "textAlign": "left",
      "verticalAlign": "middle",
      "baseline": 18,
      "strokeColor": "#121212",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "dataflow-box",
      "type": "rectangle",
      "x": 50,
      "y": 690,
      "width": 900,
      "height": 500,
      "strokeColor": "#1971c2",
      "backgroundColor": "#1a1a1a",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "stage1-box",
      "type": "rectangle",
      "x": 80,
      "y": 720,
      "width": 250,
      "height": 120,
      "strokeColor": "#2f9e44",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "stage1-text",
      "type": "text",
      "x": 100,
      "y": 730,
      "width": 210,
      "height": 100,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "üìö STAGE 1: SFT\n\nInput: GPT-2 base model\n       + Alpaca instructions\n\nProcess: Fine-tune on\n  Instruction ‚Üí Response pairs\n\nOutput: SFT Model\n  (follows instructions)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 95,
      "strokeColor": "#2f9e44",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "arrow1",
      "type": "arrow",
      "x": 340,
      "y": 780,
      "width": 40,
      "height": 0,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 3,
      "roughness": 0,
      "opacity": 100,
      "startBinding": null,
      "endBinding": null,
      "points": [[0, 0], [40, 0]],
      "lastCommittedPoint": [40, 0]
    },
    {
      "id": "stage2-box",
      "type": "rectangle",
      "x": 390,
      "y": 720,
      "width": 250,
      "height": 120,
      "strokeColor": "#9c36b5",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "stage2-text",
      "type": "text",
      "x": 410,
      "y": 730,
      "width": 210,
      "height": 100,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "üèÜ STAGE 2: Reward Model\n\nInput: SFT model\n       + HH-RLHF preferences\n\nProcess: Learn to score\n  chosen > rejected\n\nOutput: Reward Model\n  (judges quality)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 95,
      "strokeColor": "#9c36b5",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "arrow2",
      "type": "arrow",
      "x": 650,
      "y": 780,
      "width": 40,
      "height": 0,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 3,
      "roughness": 0,
      "opacity": 100,
      "startBinding": null,
      "endBinding": null,
      "points": [[0, 0], [40, 0]],
      "lastCommittedPoint": [40, 0]
    },
    {
      "id": "stage3-box",
      "type": "rectangle",
      "x": 700,
      "y": 720,
      "width": 220,
      "height": 120,
      "strokeColor": "#1971c2",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "stage3-text",
      "type": "text",
      "x": 720,
      "y": 730,
      "width": 180,
      "height": 100,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "üöÄ STAGE 3: PPO\n\nInput: SFT + Reward\n\nProcess: Generate ‚Üí\n  Score ‚Üí Update\n  (stay close to SFT)\n\nOutput: RLHF Model\n  (aligned!)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 95,
      "strokeColor": "#1971c2",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "ppo-detail-box",
      "type": "rectangle",
      "x": 80,
      "y": 870,
      "width": 840,
      "height": 290,
      "strokeColor": "#f08c00",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "ppo-detail-title",
      "type": "text",
      "x": 100,
      "y": 880,
      "width": 800,
      "height": 20,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "üîÑ PPO Training Loop (Detailed):",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 15,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "ppo-step1",
      "type": "rectangle",
      "x": 100,
      "y": 920,
      "width": 180,
      "height": 80,
      "strokeColor": "#2f9e44",
      "backgroundColor": "#121212",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "ppo-step1-text",
      "type": "text",
      "x": 110,
      "y": 930,
      "width": 160,
      "height": 60,
      "fontSize": 12,
      "fontFamily": 1,
      "text": "1. GENERATE\nPrompt ‚Üí Policy ‚Üí\nResponse\n\n(Temperature=0.7)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 55,
      "strokeColor": "#2f9e44",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "ppo-arrow1",
      "type": "arrow",
      "x": 290,
      "y": 960,
      "width": 30,
      "height": 0,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100,
      "points": [[0, 0], [30, 0]]
    },
    {
      "id": "ppo-step2",
      "type": "rectangle",
      "x": 330,
      "y": 920,
      "width": 180,
      "height": 80,
      "strokeColor": "#9c36b5",
      "backgroundColor": "#121212",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "ppo-step2-text",
      "type": "text",
      "x": 340,
      "y": 930,
      "width": 160,
      "height": 60,
      "fontSize": 12,
      "fontFamily": 1,
      "text": "2. REWARD\nResponse ‚Üí\nReward Model ‚Üí\nScore\n\n(r = 2.5)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 55,
      "strokeColor": "#9c36b5",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "ppo-arrow2",
      "type": "arrow",
      "x": 520,
      "y": 960,
      "width": 30,
      "height": 0,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100,
      "points": [[0, 0], [30, 0]]
    },
    {
      "id": "ppo-step3",
      "type": "rectangle",
      "x": 560,
      "y": 920,
      "width": 180,
      "height": 80,
      "strokeColor": "#e03131",
      "backgroundColor": "#121212",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "ppo-step3-text",
      "type": "text",
      "x": 570,
      "y": 930,
      "width": 160,
      "height": 60,
      "fontSize": 12,
      "fontFamily": 1,
      "text": "3. KL PENALTY\nCompute KL vs SFT\n\ntotal = r - Œ≤*KL\n(Œ≤ = 0.05)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 55,
      "strokeColor": "#e03131",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "ppo-arrow3",
      "type": "arrow",
      "x": 750,
      "y": 960,
      "width": 30,
      "height": 0,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100,
      "points": [[0, 0], [30, 0]]
    },
    {
      "id": "ppo-step4",
      "type": "rectangle",
      "x": 790,
      "y": 920,
      "width": 120,
      "height": 80,
      "strokeColor": "#1971c2",
      "backgroundColor": "#121212",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "ppo-step4-text",
      "type": "text",
      "x": 800,
      "y": 930,
      "width": 100,
      "height": 60,
      "fontSize": 12,
      "fontFamily": 1,
      "text": "4. UPDATE\nPPO\nclipped\nobjective\n\n‚Üí Better!",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 55,
      "strokeColor": "#1971c2",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "ppo-loop-arrow",
      "type": "arrow",
      "x": 850,
      "y": 1010,
      "width": 750,
      "height": 100,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100,
      "points": [[0, 0], [0, 40], [-750, 40], [-750, -60]],
      "startArrowhead": null,
      "endArrowhead": "arrow"
    },
    {
      "id": "ppo-loop-label",
      "type": "text",
      "x": 400,
      "y": 1040,
      "width": 200,
      "height": 20,
      "fontSize": 14,
      "fontFamily": 1,
      "text": "Repeat 100 iterations ‚Üí",
      "textAlign": "center",
      "verticalAlign": "top",
      "baseline": 15,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "ppo-formula-box",
      "type": "rectangle",
      "x": 100,
      "y": 1020,
      "width": 800,
      "height": 120,
      "strokeColor": "#9c36b5",
      "backgroundColor": "#0a0a0a",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "ppo-formula",
      "type": "text",
      "x": 120,
      "y": 1030,
      "width": 760,
      "height": 90,
      "fontSize": 13,
      "fontFamily": 3,
      "text": "üìê Key Formulas:\n\n‚Ä¢ Bradley-Terry Loss:    loss = -log(œÉ(r_chosen - r_rejected))\n‚Ä¢ KL Divergence:          KL = (log_prob_policy - log_prob_ref).sum()\n‚Ä¢ Total Reward:           reward_total = reward_model - Œ≤ * KL\n‚Ä¢ PPO Objective:          L = min(ratio * A, clip(ratio, 1-Œµ, 1+Œµ) * A)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 85,
      "strokeColor": "#9c36b5",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "section3-header",
      "type": "rectangle",
      "x": 50,
      "y": 1230,
      "width": 900,
      "height": 50,
      "strokeColor": "#2f9e44",
      "backgroundColor": "#2f9e44",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "section3-title",
      "type": "text",
      "x": 60,
      "y": 1245,
      "width": 880,
      "height": 25,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "3Ô∏è‚É£ KEY CONCEPTS: RLHF Components & Hyperparameters",
      "textAlign": "left",
      "verticalAlign": "middle",
      "baseline": 18,
      "strokeColor": "#121212",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "concepts-box",
      "type": "rectangle",
      "x": 50,
      "y": 1290,
      "width": 900,
      "height": 520,
      "strokeColor": "#2f9e44",
      "backgroundColor": "#1a1a1a",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "concept1-box",
      "type": "rectangle",
      "x": 80,
      "y": 1320,
      "width": 410,
      "height": 220,
      "strokeColor": "#f08c00",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "concept1-title",
      "type": "text",
      "x": 100,
      "y": 1330,
      "width": 370,
      "height": 20,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "üéØ RLHF (Reinforcement Learning from Human Feedback)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 15,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "concept1-text",
      "type": "text",
      "x": 100,
      "y": 1360,
      "width": 370,
      "height": 160,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "What: Train AI to align with human preferences\n\nWhy: Base LLMs predict next token but don't\n     optimize for helpfulness/harmlessness\n\nHow: 3-stage pipeline:\n  1. SFT: Teach instruction-following\n  2. Reward: Learn what humans prefer\n  3. PPO: Optimize policy for reward\n\nBreakthrough: Powers ChatGPT, Claude, Bard",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 155,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "concept2-box",
      "type": "rectangle",
      "x": 510,
      "y": 1320,
      "width": 410,
      "height": 220,
      "strokeColor": "#9c36b5",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "concept2-title",
      "type": "text",
      "x": 530,
      "y": 1330,
      "width": 370,
      "height": 20,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "üèÜ Reward Model (Bradley-Terry Preference Learning)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 15,
      "strokeColor": "#9c36b5",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "concept2-text",
      "type": "text",
      "x": 530,
      "y": 1360,
      "width": 370,
      "height": 160,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "Purpose: Scalar function that scores responses\n\nInput: Text response\nOutput: Reward (higher = better)\n\nTraining Data: Preference pairs\n  {prompt, chosen, rejected}\n\nLoss: -log(œÉ(r_chosen - r_rejected))\n  ‚Üí Maximize P(chosen > rejected)\n\nSuccess: >65% validation accuracy",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 155,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "concept3-box",
      "type": "rectangle",
      "x": 80,
      "y": 1560,
      "width": 410,
      "height": 220,
      "strokeColor": "#1971c2",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "concept3-title",
      "type": "text",
      "x": 100,
      "y": 1570,
      "width": 370,
      "height": 20,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "üöÄ PPO (Proximal Policy Optimization)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 15,
      "strokeColor": "#1971c2",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "concept3-text",
      "type": "text",
      "x": 100,
      "y": 1600,
      "width": 370,
      "height": 160,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "Purpose: Optimize policy without going too far\n\nClipped Objective: Prevents large updates\n  L = min(ratio*A, clip(ratio, 1-Œµ, 1+Œµ)*A)\n  Œµ = 0.2 (clip range)\n\nKL Penalty: Stay close to reference model\n  reward_total = r_model - Œ≤ * KL\n  Œ≤ = 0.05 (tunable)\n\nBatch: 16 prompts, 4 PPO epochs per batch",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 155,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "concept4-box",
      "type": "rectangle",
      "x": 510,
      "y": 1560,
      "width": 410,
      "height": 220,
      "strokeColor": "#e03131",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "concept4-title",
      "type": "text",
      "x": 530,
      "y": 1570,
      "width": 370,
      "height": 20,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "‚öôÔ∏è Critical Hyperparameters",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 15,
      "strokeColor": "#e03131",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "concept4-text",
      "type": "text",
      "x": 530,
      "y": 1600,
      "width": 370,
      "height": 160,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "SFT:\n  ‚Ä¢ Learning rate: 2e-5\n  ‚Ä¢ Epochs: 3\n  ‚Ä¢ Target loss: ~2.0\n\nReward Model:\n  ‚Ä¢ Learning rate: 1e-5\n  ‚Ä¢ Target accuracy: >65%\n\nPPO:\n  ‚Ä¢ Œ≤ (KL penalty): 0.01-0.1 (tune carefully!)\n  ‚Ä¢ Learning rate: 1e-6 (very small)\n  ‚Ä¢ Target KL: <0.5 (prevent drift)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 155,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "section4-header",
      "type": "rectangle",
      "x": 50,
      "y": 1850,
      "width": 900,
      "height": 50,
      "strokeColor": "#9c36b5",
      "backgroundColor": "#9c36b5",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "section4-title",
      "type": "text",
      "x": 60,
      "y": 1865,
      "width": 880,
      "height": 25,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "4Ô∏è‚É£ TIMELINE: 3-Week Project Milestones",
      "textAlign": "left",
      "verticalAlign": "middle",
      "baseline": 18,
      "strokeColor": "#121212",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "timeline-box",
      "type": "rectangle",
      "x": 50,
      "y": 1910,
      "width": 900,
      "height": 400,
      "strokeColor": "#9c36b5",
      "backgroundColor": "#1a1a1a",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "timeline-line",
      "type": "line",
      "x": 100,
      "y": 2050,
      "width": 800,
      "height": 0,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 4,
      "roughness": 0,
      "opacity": 100,
      "points": [[0, 0], [800, 0]]
    },
    {
      "id": "week1-marker",
      "type": "ellipse",
      "x": 90,
      "y": 2040,
      "width": 20,
      "height": 20,
      "strokeColor": "#2f9e44",
      "backgroundColor": "#2f9e44",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "week1-box",
      "type": "rectangle",
      "x": 80,
      "y": 1950,
      "width": 240,
      "height": 80,
      "strokeColor": "#2f9e44",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "week1-text",
      "type": "text",
      "x": 90,
      "y": 1960,
      "width": 220,
      "height": 60,
      "fontSize": 12,
      "fontFamily": 1,
      "text": "üìö WEEK 1: SFT Training\n\nDay 1-2: Load Alpaca dataset\nDay 3-4: TODO 1 - SFT loss\nDay 5: Evaluate (loss ~2.0)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 55,
      "strokeColor": "#2f9e44",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "week1-label",
      "type": "text",
      "x": 70,
      "y": 2070,
      "width": 60,
      "height": 20,
      "fontSize": 14,
      "fontFamily": 1,
      "text": "Week 1",
      "textAlign": "center",
      "verticalAlign": "top",
      "baseline": 15,
      "strokeColor": "#2f9e44",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "week2-marker",
      "type": "ellipse",
      "x": 355,
      "y": 2040,
      "width": 20,
      "height": 20,
      "strokeColor": "#9c36b5",
      "backgroundColor": "#9c36b5",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "week2-box",
      "type": "rectangle",
      "x": 340,
      "y": 1950,
      "width": 250,
      "height": 80,
      "strokeColor": "#9c36b5",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "week2-text",
      "type": "text",
      "x": 350,
      "y": 1960,
      "width": 230,
      "height": 60,
      "fontSize": 12,
      "fontFamily": 1,
      "text": "üèÜ WEEK 2: Reward Model\n\nDay 6-7: Load HH-RLHF dataset\nDay 8-9: TODO 2 - Bradley-Terry\nDay 10: Evaluate (accuracy >65%)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 55,
      "strokeColor": "#9c36b5",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "week2-label",
      "type": "text",
      "x": 335,
      "y": 2070,
      "width": 60,
      "height": 20,
      "fontSize": 14,
      "fontFamily": 1,
      "text": "Week 2",
      "textAlign": "center",
      "verticalAlign": "top",
      "baseline": 15,
      "strokeColor": "#9c36b5",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "week3-marker",
      "type": "ellipse",
      "x": 620,
      "y": 2040,
      "width": 20,
      "height": 20,
      "strokeColor": "#1971c2",
      "backgroundColor": "#1971c2",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "week3-box",
      "type": "rectangle",
      "x": 610,
      "y": 1950,
      "width": 300,
      "height": 80,
      "strokeColor": "#1971c2",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "week3-text",
      "type": "text",
      "x": 620,
      "y": 1960,
      "width": 280,
      "height": 60,
      "fontSize": 12,
      "fontFamily": 1,
      "text": "üöÄ WEEK 3: PPO + Evaluation\n\nDay 11-13: TODO 3 - KL penalty PPO\nDay 14-17: TODO 4 - Automated eval\nDay 18-21: Demo + Portfolio + Report",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 55,
      "strokeColor": "#1971c2",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "week3-label",
      "type": "text",
      "x": 600,
      "y": 2070,
      "width": 60,
      "height": 20,
      "fontSize": 14,
      "fontFamily": 1,
      "text": "Week 3",
      "textAlign": "center",
      "verticalAlign": "top",
      "baseline": 15,
      "strokeColor": "#1971c2",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "deliverables-box",
      "type": "rectangle",
      "x": 80,
      "y": 2120,
      "width": 840,
      "height": 160,
      "strokeColor": "#f08c00",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "deliverables-title",
      "type": "text",
      "x": 100,
      "y": 2130,
      "width": 800,
      "height": 20,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "üì¶ Final Deliverables:",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 15,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "deliverables-text",
      "type": "text",
      "x": 100,
      "y": 2160,
      "width": 800,
      "height": 100,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "‚úÖ Completed notebook (4 TODOs solved)\n‚úÖ 3 trained models (SFT checkpoint, reward model, RLHF final)\n‚úÖ Evaluation report (win rate ‚â•75%, training curves, reward hacking analysis)\n‚úÖ Interactive Gradio demo (chatbot with base vs SFT vs RLHF comparison)\n‚úÖ Portfolio website update (\"Aligned LLM with RLHF - 78% improvement\")",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 95,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "section5-header",
      "type": "rectangle",
      "x": 50,
      "y": 2350,
      "width": 900,
      "height": 50,
      "strokeColor": "#e03131",
      "backgroundColor": "#e03131",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "section5-title",
      "type": "text",
      "x": 60,
      "y": 2365,
      "width": 880,
      "height": 25,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "5Ô∏è‚É£ COMMON ERRORS: Debugging RLHF Training Issues",
      "textAlign": "left",
      "verticalAlign": "middle",
      "baseline": 18,
      "strokeColor": "#121212",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "errors-box",
      "type": "rectangle",
      "x": 50,
      "y": 2410,
      "width": 900,
      "height": 540,
      "strokeColor": "#e03131",
      "backgroundColor": "#1a1a1a",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "error1-box",
      "type": "rectangle",
      "x": 80,
      "y": 2440,
      "width": 840,
      "height": 100,
      "strokeColor": "#f08c00",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "error1-text",
      "type": "text",
      "x": 100,
      "y": 2450,
      "width": 800,
      "height": 80,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "‚ùå ERROR: Reward Hacking (rewards spike but quality doesn't improve)\n\nCause: Model exploits reward model instead of genuinely improving\nSymptoms: Sudden reward jumps, KL divergence >0.5, nonsensical outputs\nFix: Increase Œ≤ (KL penalty) from 0.05 ‚Üí 0.1 or higher\n     Add length penalty to prevent length gaming",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 75,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "error2-box",
      "type": "rectangle",
      "x": 80,
      "y": 2560,
      "width": 840,
      "height": 100,
      "strokeColor": "#9c36b5",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "error2-text",
      "type": "text",
      "x": 100,
      "y": 2570,
      "width": 800,
      "height": 80,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "‚ùå ERROR: KL Divergence Exploding (KL >1.0 during PPO)\n\nCause: Policy drifting too far from reference model\nSymptoms: KL divergence increases rapidly, model outputs degrade\nFix: Increase Œ≤ to 0.2+, reduce PPO learning rate to 5e-7\n     Reduce PPO epochs per batch from 4 ‚Üí 2",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 75,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "error3-box",
      "type": "rectangle",
      "x": 80,
      "y": 2680,
      "width": 840,
      "height": 100,
      "strokeColor": "#2f9e44",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "error3-text",
      "type": "text",
      "x": 100,
      "y": 2690,
      "width": 800,
      "height": 80,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "‚ùå ERROR: Reward Model Stuck at 50% Accuracy (random guessing)\n\nCause: Model not learning from preference data\nSymptoms: Validation accuracy stays at ~50%, reward distributions don't separate\nFix: Check data format (chosen vs rejected labels correct?)\n     Lower learning rate to 5e-6, increase epochs, filter noisy pairs",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 75,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "error4-box",
      "type": "rectangle",
      "x": 80,
      "y": 2800,
      "width": 840,
      "height": 100,
      "strokeColor": "#1971c2",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "error4-text",
      "type": "text",
      "x": 100,
      "y": 2810,
      "width": 800,
      "height": 80,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "‚ùå ERROR: RLHF Model Worse Than SFT (negative improvement)\n\nCause: Reward model inaccurate or PPO overfitting to reward model\nSymptoms: Win rate <50%, RLHF outputs lower quality than SFT\nFix: Retrain reward model with more data, stop PPO earlier (use validation)\n     Check if reward model correlates with human judgment",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 75,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "section6-header",
      "type": "rectangle",
      "x": 50,
      "y": 2990,
      "width": 900,
      "height": 50,
      "strokeColor": "#1971c2",
      "backgroundColor": "#1971c2",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "section6-title",
      "type": "text",
      "x": 60,
      "y": 3005,
      "width": 880,
      "height": 25,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "6Ô∏è‚É£ DEBUG CHECKLIST: Systematic RLHF Troubleshooting",
      "textAlign": "left",
      "verticalAlign": "middle",
      "baseline": 18,
      "strokeColor": "#121212",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "debug-box",
      "type": "rectangle",
      "x": 50,
      "y": 3050,
      "width": 900,
      "height": 480,
      "strokeColor": "#1971c2",
      "backgroundColor": "#1a1a1a",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "debug-sft",
      "type": "rectangle",
      "x": 80,
      "y": 3080,
      "width": 400,
      "height": 180,
      "strokeColor": "#2f9e44",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "debug-sft-title",
      "type": "text",
      "x": 100,
      "y": 3090,
      "width": 360,
      "height": 20,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "üìö Stage 1: SFT Checklist",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 15,
      "strokeColor": "#2f9e44",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "debug-sft-text",
      "type": "text",
      "x": 100,
      "y": 3120,
      "width": 360,
      "height": 130,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "‚úì Dataset loaded? (52K Alpaca examples)\n‚úì Loss decreasing? (3.5 ‚Üí 2.0 over epochs)\n‚úì Sample generations coherent?\n‚úì Model follows instruction format?\n‚úì Learning rate tuned? (try 1e-5 to 5e-5)\n‚úì Validation loss similar to train?\n‚úì SFT checkpoint saved?",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 125,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "debug-rm",
      "type": "rectangle",
      "x": 500,
      "y": 3080,
      "width": 420,
      "height": 180,
      "strokeColor": "#9c36b5",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "debug-rm-title",
      "type": "text",
      "x": 520,
      "y": 3090,
      "width": 380,
      "height": 20,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "üèÜ Stage 2: Reward Model Checklist",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 15,
      "strokeColor": "#9c36b5",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "debug-rm-text",
      "type": "text",
      "x": 520,
      "y": 3120,
      "width": 380,
      "height": 130,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "‚úì Preference data correct? (chosen vs rejected)\n‚úì Bradley-Terry loss implemented?\n‚úì Accuracy >65%? (else retrain longer)\n‚úì Reward distributions separate?\n  (plot r_chosen vs r_rejected histograms)\n‚úì Reward head initialized? (std=0.01)\n‚úì Backbone frozen? (only train head)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 125,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "debug-ppo",
      "type": "rectangle",
      "x": 80,
      "y": 3280,
      "width": 840,
      "height": 220,
      "strokeColor": "#f08c00",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "debug-ppo-title",
      "type": "text",
      "x": 100,
      "y": 3290,
      "width": 800,
      "height": 20,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "üöÄ Stage 3: PPO Checklist",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 15,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "debug-ppo-text",
      "type": "text",
      "x": 100,
      "y": 3320,
      "width": 800,
      "height": 170,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "‚úì KL penalty implemented? (reward_total = r - Œ≤ * KL)\n‚úì KL divergence <0.5? (if not, increase Œ≤)\n‚úì Rewards increasing over iterations? (plot reward curve)\n‚úì Policy loss and value loss converging?\n‚úì Reference model frozen? (SFT checkpoint for KL comparison)\n‚úì Sample outputs improving? (check every 10 iterations)\n‚úì Reward hacking detected? (use detect_reward_hacking() from utils)\n‚úì Training stable? (no NaN losses, no gradient explosions)\n‚úì Win rate ‚â•75% vs SFT? (run evaluation on held-out prompts)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 165,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "section7-header",
      "type": "rectangle",
      "x": 50,
      "y": 3570,
      "width": 900,
      "height": 50,
      "strokeColor": "#2f9e44",
      "backgroundColor": "#2f9e44",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "section7-title",
      "type": "text",
      "x": 60,
      "y": 3585,
      "width": 880,
      "height": 25,
      "fontSize": 20,
      "fontFamily": 1,
      "text": "7Ô∏è‚É£ MENTAL MODEL: Training an AI with Human Taste",
      "textAlign": "left",
      "verticalAlign": "middle",
      "baseline": 18,
      "strokeColor": "#121212",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "mental-box",
      "type": "rectangle",
      "x": 50,
      "y": 3630,
      "width": 900,
      "height": 480,
      "strokeColor": "#2f9e44",
      "backgroundColor": "#1a1a1a",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "analogy-box",
      "type": "rectangle",
      "x": 80,
      "y": 3660,
      "width": 840,
      "height": 200,
      "strokeColor": "#f08c00",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "analogy-title",
      "type": "text",
      "x": 100,
      "y": 3670,
      "width": 800,
      "height": 25,
      "fontSize": 18,
      "fontFamily": 1,
      "text": "üé® The Restaurant Analogy: Training a Chef with Customer Feedback",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 20,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "analogy-text",
      "type": "text",
      "x": 100,
      "y": 3710,
      "width": 800,
      "height": 130,
      "fontSize": 14,
      "fontFamily": 1,
      "text": "üìö SFT = Cooking school (learn basic recipes from cookbook)\n   ‚Üí Chef can follow recipes but doesn't know what tastes good\n\nüèÜ Reward Model = Food critic (learns customer preferences)\n   ‚Üí \"Dish A better than Dish B\" ‚Üí learns what makes food delicious\n\nüöÄ PPO = Practice with feedback (cook ‚Üí critic scores ‚Üí improve)\n   ‚Üí BUT: Don't change style too much! (KL penalty = stay true to basics)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 125,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "intuition-box",
      "type": "rectangle",
      "x": 80,
      "y": 3880,
      "width": 410,
      "height": 200,
      "strokeColor": "#1971c2",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "intuition-title",
      "type": "text",
      "x": 100,
      "y": 3890,
      "width": 370,
      "height": 20,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "üí° Core Intuition: Why 3 Stages?",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 15,
      "strokeColor": "#1971c2",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "intuition-text",
      "type": "text",
      "x": 100,
      "y": 3920,
      "width": 370,
      "height": 150,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "Base Model: Predicts next token\n  (no concept of \"helpful\" or \"good\")\n\n‚ùì Problem: Can't directly optimize for\n   human preferences (no gradient!)\n\nüí° Solution: 3-stage pipeline\n  1. SFT: Warm start (instruction format)\n  2. Reward: Proxy for human judgment\n  3. PPO: Optimize proxy while staying\n          close to SFT (avoid overfitting)",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 145,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "key-insight-box",
      "type": "rectangle",
      "x": 510,
      "y": 3880,
      "width": 410,
      "height": 200,
      "strokeColor": "#9c36b5",
      "backgroundColor": "#0d0d0d",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "roughness": 1,
      "opacity": 100
    },
    {
      "id": "key-insight-title",
      "type": "text",
      "x": 530,
      "y": 3890,
      "width": 370,
      "height": 20,
      "fontSize": 16,
      "fontFamily": 1,
      "text": "üîë Key Insight: The KL Penalty is Critical!",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 15,
      "strokeColor": "#9c36b5",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    },
    {
      "id": "key-insight-text",
      "type": "text",
      "x": 530,
      "y": 3920,
      "width": 370,
      "height": 150,
      "fontSize": 13,
      "fontFamily": 1,
      "text": "Without KL penalty:\n  ‚Üí Model games reward (\"reward hacking\")\n  ‚Üí Outputs nonsense that scores high\n  ‚Üí Example: Very long repetitive text\n\nWith KL penalty (Œ≤ = 0.05):\n  ‚Üí reward_total = r - Œ≤ * KL(policy || ref)\n  ‚Üí Policy can't drift too far from SFT\n  ‚Üí Forces genuine improvement\n\nüéØ Tuning Œ≤ is an art: Too low = hacking,\n   too high = no improvement",
      "textAlign": "left",
      "verticalAlign": "top",
      "baseline": 145,
      "strokeColor": "#ffffff",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 1,
      "roughness": 0,
      "opacity": 100
    }
  ],
  "appState": {
    "gridSize": null,
    "viewBackgroundColor": "#121212"
  },
  "files": {}
}
